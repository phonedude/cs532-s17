\documentclass[a4paper, 11pt]{article}
\usepackage{comment} % enables the use of multi-line comments (\ifx \fi) 
\usepackage{xparse}% http://ctan.org/pkg/xparse
\NewDocumentCommand{\Log}{o}{%
  \IfNoValueTF{#1}{}{{}^{#1}\!}\log}%
  
\usepackage{fullpage} % changes the margin
\usepackage{longtable}
\usepackage{graphicx}
\usepackage{fancyvrb,xcolor}
\usepackage{listings}
\usepackage{color}
\usepackage[hyphenbreaks]{breakurl}
\usepackage[hyphens]{url}
\usepackage[margin=3cm]{geometry}
\usepackage{relsize}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\usepackage{float}
\usepackage{caption}
\DeclareCaptionFont{white}{\color{white}}
\DeclareCaptionFormat{listing}{\colorbox{gray}{\parbox{\textwidth}{#1#2#3}}}
\captionsetup[lstlisting]{format=listing,labelfont=white,textfont=white}
\newcommand{\bigqm}[1][1]{\text{\larger[#1]{\textbf{?}}}}
\lstset{
  language=Java,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}
\graphicspath{ {images/} }

\begin{document}
%Header-Make sure you update this information!!!!
\noindent
\large\textbf{Assignment 3} \hfill \textbf{Hussam Hallak} \\
\normalsize CS532, Web Science, Spring 2017\hfill CS Master's Student \\
Old Dominion University, Computer Science Dept \hfill Prof: Dr. Nelson 

\section*{Question 1:}
Download the 1000 URIs from assignment \#2.  ``curl'', ``wget'', or
``lynx'' are all good candidate programs to use.  We want just the
raw HTML, not the images, stylesheets, etc.

from the command line:
\begin{lstlisting}[language=bash,label=Command:, breakatwhitespace=〈false), caption=Command:]
% curl http://www.cnn.com/ > www.cnn.com

% wget -O www.cnn.com http://www.cnn.com/

% lynx -source http://www.cnn.com/ > www.cnn.com
\end{lstlisting}

``www.cnn.com'' is just an example output file name, keep in mind
that the shell will not like some of the characters that can occur
in URIs (e.g., ``$?$'', ``\&'').  You might want to hash the URIs, like:
from the command line:
\begin{lstlisting}[language=bash,label=Command:, breakatwhitespace=〈false), caption=Command:]
% echo -n "http://www.cs.odu.edu/show_features.shtml?72" | md5
41d5f125d13b4bb554e6e31b6b591eeb
\end{lstlisting}
(``md5sum'' on some machines; note the ``-n'' in echo -- this removes
the trailing newline.) 
\linebreak
\linebreak
\noindent
Now use a tool to remove (most) of the HTML markup.  ``lynx'' will
do a fair job:
\begin{lstlisting}[language=bash,label=Command:, breakatwhitespace=〈false), caption=Command:]
% lynx -dump -force_html www.cnn.com > www.cnn.com.processed
\end{lstlisting}
Use another (better) tool if you know of one.  
\noindent
A "better" approach is to use BeautifulSoup, see:

http://stackoverflow.com/questions/1936466/beautifulsoup-grab-visible-webpage-text

\noindent
for some hints on how to start.  Note that none of these methods 
are going to be perfect.
\linebreak
\linebreak
\noindent
Keep both files for each URI (i.e., raw HTML and processed). 
Upload both sets of files to your github account.


\subsection*{Answer:}
The approach is divided into two steps:

1. Take the unique links collected from Assignment 2, saved in ``uniquelinks.txt'' and download their content, then save it to text files, one file for each link. For that purpose I used md5 hashing to name the output files.

2. Remove (most) of the HTML markup from the content of the downloaded HTML pages.

I wrote a python script that will combine both steps. First it will download the raw HTML for each link and save it to a file, then it will strip HTML markup and save it to another file with the same name postfixed with ``.processed''. The program will also create a text file named ``map.txt'' to map each link to its new file name. That will make the work easier for Question 2.

\lstinputlisting[language=Python, breakatwhitespace=〈false), label=parselinks.py, caption=The content of parselinks.py]{Q1/parselinks.py}
\pagebreak

\begin{lstlisting}[language=bash, breakatwhitespace=〈false), label=running parselinks.py, caption= Running parselinks.py to download raw HTML and visible text from URIs]
root@ima-app:/var/www/Hussam/A3# python parselinks.py uniquelinks.txt
root@ima-app:/var/www/Hussam/A3# ls | wc -w
798
\end{lstlisting}


The program created 794 output files, two files for each link. This means that the content of 397 URIs was downloaded and processed successfully. The rest of the links generated errors. That could be due to different reasons including bad HTML markup, the page no longer exists on the web, or other reasons. 


\subsection*{Included Files:}
parselinks.py, uniquelinks.txt, map.txt

\noindent
Folder ``raw'' contains all raw HTML files
 
\noindent
Folder`` processed'' contains all visible text in HTML files

\section*{Question 2:}
Choose a query term (e.g., "shadow") that is not a stop word
(see week 5 slides) and not HTML markup from step 1 (e.g., "http")
that matches at least 10 documents (hint: use "grep" on the processed
files).  If the term is present in more than 10 documents, choose
any 10 from your list.  (If you do not end up with a list of 10
URIs, you've done something wrong).

As per the example in the week 5 slides, compute TFIDF values for
the term in each of the 10 documents and create a table with the
TF, IDF, and TFIDF values, as well as the corresponding URIs.  The
URIs will be ranked in decreasing order by TFIDF values.  For
example:

Table 1. 10 Hits for the term "shadow", ranked by TFIDF.

TFIDF TF IDF URI

--------------------------------

0.150	0.014	10.680	http://foo.com/

0.044	0.008	10.680	http://bar.com/


You can use Google or Bing for the DF estimation.  To count the
number of words in the processed document (i.e., the deonminator
for TF), you can use "wc":

% wc -w www.cnn.com.processed
    2370 www.cnn.com.processed

It won't be completely accurate, but it will be probably be
consistently inaccurate across all files.  You can use more 
accurate methods if you'd like, just explain how you did it.  

Don't forget the log base 2 for IDF, and mind your significant
digits!

\burl{https://en.wikipedia.org/wiki/Significant_figures#Rounding_and_decimal_places}

\subsection*{Answer:}

I chose the query term ``blue'' and that matched exactly 10 documents from my output file in Question 1.

\begin{lstlisting}[language=bash, breakatwhitespace=〈false), label=query term search, caption= Results from searching for the chosen query term]
root@ima-app:/var/www/Hussam/A3# grep -il "blue" *.processed
2abb9ccc9ac936b183b0ba3514bb246a.processed
4aeb25630e7cc76a102b5f35a5bee8af.processed
7a83161b2ccc2c6d0c5c74dcfc247164.processed
866df289fc6c7d73ddcbaf35199a358d.processed
8e72632b606ebcb16f1f83a1f248bcea.processed
913f142ab3ab5704e42b195c50116d9c.processed
e1577b08960791988e2ccb0315148f30.processed
f158b3a1dabb47a8a42fe06cd86444c0.processed
fc43d6cca315eda1428c7b4fdaf8cf77.processed
fe93efbb7ec52521f2a32784a5563385.processed
root@ima-app:/var/www/Hussam/A3# grep -il "germ" *.processed | wc -l
10
root@ima-app:/var/www/Hussam/A3#

\end{lstlisting}

I created a new directory named ``blue'' and copied all processed documents that have the query term to that folder.

\begin{lstlisting}[language=bash, breakatwhitespace=〈false), label=copy docs, caption= Copying the documents containing the query term to a separate folder]
root@ima-app:/var/www/Hussam/A3# mkdir blue
root@ima-app:/var/www/Hussam/A3# cp `grep -il "blue" *.processed` blue
root@ima-app:/var/www/Hussam/A3# cd blue
root@ima-app:/var/www/Hussam/A3/blue# ls
2abb9ccc9ac936b183b0ba3514bb246a.processed
4aeb25630e7cc76a102b5f35a5bee8af.processed
7a83161b2ccc2c6d0c5c74dcfc247164.processed
866df289fc6c7d73ddcbaf35199a358d.processed
8e72632b606ebcb16f1f83a1f248bcea.processed
913f142ab3ab5704e42b195c50116d9c.processed
e1577b08960791988e2ccb0315148f30.processed
f158b3a1dabb47a8a42fe06cd86444c0.processed
fc43d6cca315eda1428c7b4fdaf8cf77.processed
fe93efbb7ec52521f2a32784a5563385.processed
root@ima-app:/var/www/Hussam/A3/blue#
\end{lstlisting}

It's time to calculate TFIDF, TF, IDF for the query term ``blue'' and construct the requested table. I will use the file map.txt, created in question 1 to find out which processed file belongs to which URI.

Computing TF:

\begin{lstlisting}[language=bash, breakatwhitespace=〈false), label=Computing TF, caption= Computing TF:]
root@ima-app:/var/www/Hussam/A3/blue# grep -io "blue" 2abb9ccc9ac936b183b0ba3514bb246a.processed | wc -w
2
root@ima-app:/var/www/Hussam/A3/blue# grep -io "blue" 4aeb25630e7cc76a102b5f35a5bee8af.processed | wc -w
1
root@ima-app:/var/www/Hussam/A3/blue# grep -io "blue" 7a83161b2ccc2c6d0c5c74dcfc247164.processed | wc -w
1
root@ima-app:/var/www/Hussam/A3/blue# grep -io "blue" 866df289fc6c7d73ddcbaf35199a358d.processed | wc -w
4
root@ima-app:/var/www/Hussam/A3/blue# grep -io "blue" 8e72632b606ebcb16f1f83a1f248bcea.processed | wc -w
1
root@ima-app:/var/www/Hussam/A3/blue# grep -io "blue" 913f142ab3ab5704e42b195c50116d9c.processed | wc -w
1
root@ima-app:/var/www/Hussam/A3/blue# grep -io "blue" e1577b08960791988e2ccb0315148f30.processed | wc -w
1
root@ima-app:/var/www/Hussam/A3/blue# grep -io "blue" f158b3a1dabb47a8a42fe06cd86444c0.processed | wc -w
2
root@ima-app:/var/www/Hussam/A3/blue# grep -io "blue" fc43d6cca315eda1428c7b4fdaf8cf77.processed | wc -w
1
root@ima-app:/var/www/Hussam/A3/blue# grep -io "blue" fe93efbb7ec52521f2a32784a5563385.processed | wc -w
2
\end{lstlisting}

Normalizing TF:

TF is normalized by dividing the number of occurrences of "blue" divided by the total number of words in the document.

\begin{lstlisting}[language=bash, breakatwhitespace=〈false), label=Finding the total number of words in the documents, caption= Finding the total number of words in the documents:]
root@ima-app:/var/www/Hussam/A3/blue# wc -w 2abb9ccc9ac936b183b0ba3514bb246a.processed
1563 2abb9ccc9ac936b183b0ba3514bb246a.processed
root@ima-app:/var/www/Hussam/A3/blue# wc -w 4aeb25630e7cc76a102b5f35a5bee8af.processed
822 4aeb25630e7cc76a102b5f35a5bee8af.processed
root@ima-app:/var/www/Hussam/A3/blue# wc -w 7a83161b2ccc2c6d0c5c74dcfc247164.processed
317 7a83161b2ccc2c6d0c5c74dcfc247164.processed
root@ima-app:/var/www/Hussam/A3/blue# wc -w 866df289fc6c7d73ddcbaf35199a358d.processed
1921 866df289fc6c7d73ddcbaf35199a358d.processed
root@ima-app:/var/www/Hussam/A3/blue# wc -w 8e72632b606ebcb16f1f83a1f248bcea.processed
2214 8e72632b606ebcb16f1f83a1f248bcea.processed
root@ima-app:/var/www/Hussam/A3/blue# wc -w 913f142ab3ab5704e42b195c50116d9c.processed
2060 913f142ab3ab5704e42b195c50116d9c.processed
root@ima-app:/var/www/Hussam/A3/blue# wc -w e1577b08960791988e2ccb0315148f30.processed
572 e1577b08960791988e2ccb0315148f30.processed
root@ima-app:/var/www/Hussam/A3/blue# wc -w f158b3a1dabb47a8a42fe06cd86444c0.processed
416 f158b3a1dabb47a8a42fe06cd86444c0.processed
root@ima-app:/var/www/Hussam/A3/blue# wc -w fc43d6cca315eda1428c7b4fdaf8cf77.processed
1431 fc43d6cca315eda1428c7b4fdaf8cf77.processed
root@ima-app:/var/www/Hussam/A3/blue# wc -w fe93efbb7ec52521f2a32784a5563385.processed
1049 fe93efbb7ec52521f2a32784a5563385.processed
\end{lstlisting}

\pagebreak

Searching the file name of ``.processed'' files in ``map.txt'' will give us the corresponding URI.

\begin{lstlisting}[language=bash, breakatwhitespace=〈false), label=Matching file names to URIs, caption= Matching file names to URIs:]
root@ima-app:/var/www/Hussam/A3# grep --after-context=0 --before-context=1 "2abb9ccc9ac936b183b0ba3514bb246a.processed" map.txt
http://www.boohoo.com/
        2abb9ccc9ac936b183b0ba3514bb246a.processed
root@ima-app:/var/www/Hussam/A3# grep --after-context=0 --before-context=1 "4aeb25630e7cc76a102b5f35a5bee8af.processed" map.txt
http://www.vox.com/2016/9/7/12815076/america-uninsured-rate-dropped?source=socnet_tw_aca_20160907_bo_uninsured-rate-low_vox_3&utm_medium=social&utm_source=tw_bo&utm_campaign=socnet_tw_aca_20160907_bo_uninsured-rate-low_vox_3&utm_content=20160907_bo_uninsured-rate-low_vox_3
        4aeb25630e7cc76a102b5f35a5bee8af.processed
root@ima-app:/var/www/Hussam/A3# grep --after-context=0 --before-context=1 "7a83161b2ccc2c6d0c5c74dcfc247164.processed" map.txt
http://www.vox.com/new-money/2016/10/21/13361414/median-wages?source=socnet_tw_econ_20161026_bo_wage-increase_durable_4&utm_medium=social&utm_source=tw_bo&utm_campaign=socnet_tw_econ_20161026_bo_wage-increase_durable_4&utm_content=20161026_bo_wage-increase_durable_4
        7a83161b2ccc2c6d0c5c74dcfc247164.processed
root@ima-app:/var/www/Hussam/A3# grep --after-context=0 --before-context=1 "866df289fc6c7d73ddcbaf35199a358d.processed" map.txt
https://www.fisherwallace.com/?gclid=CNr-xNqt38UCFUU8gQodQHIAsQ
        866df289fc6c7d73ddcbaf35199a358d.processed
root@ima-app:/var/www/Hussam/A3# grep --after-context=0 --before-context=1 "8e72632b606ebcb16f1f83a1f248bcea.processed" map.txt
http://www.nme.com/features/taylor-swift-power-fame-and-the-future-the-full-nme-cover-interview-549
        8e72632b606ebcb16f1f83a1f248bcea.processed
root@ima-app:/var/www/Hussam/A3# grep --after-context=0 --before-context=1 "913f142ab3ab5704e42b195c50116d9c.processed" map.txt
http://www.health.com/health/gallery/0,,20705881,00.html
        913f142ab3ab5704e42b195c50116d9c.processed
root@ima-app:/var/www/Hussam/A3# grep --after-context=0 --before-context=1 "e1577b08960791988e2ccb0315148f30.processed" map.txt
http://www.mtv.com/vma/winners
        e1577b08960791988e2ccb0315148f30.processed
root@ima-app:/var/www/Hussam/A3# grep --after-context=0 --before-context=1 "f158b3a1dabb47a8a42fe06cd86444c0.processed" map.txt
http://www.vox.com/energy-and-environment/2016/9/30/13111088/cleantech-costs-falling-one-chart?source=socnet_tw_cc_20161006_bo_clean-energy_technology_1&utm_medium=social&utm_source=tw_bo&utm_campaign=socnet_tw_cc_20161006_bo_clean-energy_technology_1&utm_content=20161006_bo_clean-energy_technology_1
        f158b3a1dabb47a8a42fe06cd86444c0.processed
root@ima-app:/var/www/Hussam/A3# grep --after-context=0 --before-context=1 "fc43d6cca315eda1428c7b4fdaf8cf77.processed" map.txt
https://www.washingtonpost.com/news/energy-environment/wp/2016/10/10/climate-change-has-been-making-western-forest-fires-worse-for-decades-study-says/?source=socnet_tw_cc_20161027_bo_energy-environment_climate_5&utm_medium=social&utm_source=tw_bo&utm_campaign=socnet_tw_cc_20161027_bo_energy-environment_climate_5&utm_content=20161027_bo_energy-environment_climate_5
        fc43d6cca315eda1428c7b4fdaf8cf77.processed
root@ima-app:/var/www/Hussam/A3# grep --after-context=0 --before-context=1 "fe93efbb7ec52521f2a32784a5563385.processed" map.txt
http://www.npr.org/sections/thetwo-way/2016/08/31/492177267/obama-at-lake-tahoe-praises-conservation-efforts?source=socnet_tw_cc_20160906_bo_npr-article_conservation_2&utm_medium=socnet&utm_source=tw&utm_campaign=socnet_tw_cc_20160906_bo_npr-article_conservation_2&utm_content=20160906_bo_npr-article_conservation_2
        fe93efbb7ec52521f2a32784a5563385.processed
\end{lstlisting}

In the following table:

TF: The raw TF which is the number of occurrences of "blue" in a document.

WC: The total number of words in the document.

N-TF: The normalized TF for the document.

URI: The URI of the document.

Table 1:

\begin{longtable}{ |p{0.3cm}|p{0.7cm}|p{1cm}|p{11cm}| } 
 \hline
TF & WC & N-TF & URI \\
 \hline 
 2 & 1563 & 0.0013 & 
 \begin{lstlisting}[breakatwhitespace=〈false)]
 http://www.boohoo.com/
\end{lstlisting} 
 \\
 \hline 
 1 & 822 & 0.0012 & 
\begin{lstlisting}[breakatwhitespace=〈false)] 
http://www.vox.com/2016/9/7/12815076/america-uninsured-rate-dropped?source=socnet_tw_aca_20160907_bo_uninsured-rate-low_vox_3&utm_medium=social&utm_source=tw_bo&utm_campaign=socnet_tw_aca_20160907_bo_uninsured-rate-low_vox_3&utm_content=20160907_bo_uninsured-rate-low_vox_3
\end{lstlisting}
  \\ 
 \hline 
 1 & 317 & 0.0032 & 
\begin{lstlisting}[breakatwhitespace=〈false)]  
http://www.vox.com/new-money/2016/10/21/13361414/median-wages?source=socnet_tw_econ_20161026_bo_wage-increase_durable_4&utm_medium=social&utm_source=tw_bo&utm_campaign=socnet_tw_econ_20161026_bo_wage-increase_durable_4&utm_content=20161026_bo_wage-increase_durable_4 
 \end{lstlisting}
 \\
 \hline 
 4 & 1921 & 0.0021 & 
 \begin{lstlisting}[breakatwhitespace=〈false)]
https://www.fisherwallace.com/?gclid=CNr-xNqt38UCFUU8gQodQHIAsQ 
\end{lstlisting}
\\ 
 \hline
 1 & 2214 & 0.0005 & 
 \begin{lstlisting}[breakatwhitespace=〈false)] 
 http://www.nme.com/features/taylor-swift-power-fame-and-the-future-the-full-nme-cover-interview-549 
  \end{lstlisting}
 \\
 \hline 
 1 & 2060 & 0.0005 & 
 \begin{lstlisting}[breakatwhitespace=〈false)] 
http://www.health.com/health/gallery/0,,20705881,00.html 
 \end{lstlisting}
 \\ 
 \hline 
 1 & 572 & 0.0017 & 
 \begin{lstlisting}[breakatwhitespace=〈false)]
http://www.mtv.com/vma/winners
  \end{lstlisting}
  \\
 \hline 
 2 & 416 & 0.0048 & 
 \begin{lstlisting}[breakatwhitespace=〈false)] 
http://www.vox.com/energy-and-environment/2016/9/30/13111088/cleantech-costs-falling-one-chart?source=socnet_tw_cc_20161006_bo_clean-energy_technology_1&utm_medium=social&utm_source=tw_bo&utm_campaign=socnet_tw_cc_20161006_bo_clean-energy_technology_1&utm_content=20161006_bo_clean-energy_technology_1
  \end{lstlisting}
 \\ 
  \hline 
 1 & 1431 & 0.0007 &
\begin{lstlisting}[breakatwhitespace=〈false)] 
https://www.washingtonpost.com/news/energy-environment/wp/2016/10/10/climate-change-has-been-making-western-forest-fires-worse-for-decades-study-says/?source=socnet_tw_cc_20161027_bo_energy-environment_climate_5&utm_medium=social&utm_source=tw_bo&utm_campaign=socnet_tw_cc_20161027_bo_energy-environment_climate_5&utm_content=20161027_bo_energy-environment_climate_5 
\end{lstlisting}
\\
 \hline 
 2 & 1049 & 0.0019 & 
\begin{lstlisting}[breakatwhitespace=〈false)] 
http://www.npr.org/sections/thetwo-way/2016/08/31/492177267/obama-at-lake-tahoe-praises-conservation-efforts?source=socnet_tw_cc_20160906_bo_npr-article_conservation_2&utm_medium=socnet&utm_source=tw&utm_campaign=socnet_tw_cc_20160906_bo_npr-article_conservation_2&utm_content=20160906_bo_npr-article_conservation_2 
\end{lstlisting}
\\
 \hline
\end{longtable}

Calculating IDF:
According to http://www.worldwidewebsize.com/ the total number of indexed web pages in Google is 50 Billion. I searched for the word "blue" in Google and it returned About 15,520,000,000 results.

\[
  IDF("blue") = \log_2 (\frac{50B}{15.5B}) = 1.6897
\]

Now we can calculate TF-IDF for each document using this formula:
\[
TF-IDF = (TF)\times(IDF)
\]

\pagebreak

Now we can generate the required table:

Table 2:

\begin{longtable}{ |p{1cm}|p{1cm}|p{1cm}|p{10cm}| } 
 \hline
TFIDF & TF & IDF & URI \\
 \hline 
 0.0022 & 0.0013 & 1.6897 &
 \begin{lstlisting}[breakatwhitespace=〈false)]
 http://www.boohoo.com/
\end{lstlisting} 
 \\
 \hline 
 0.0020 & 0.0012 & 1.6897 &
\begin{lstlisting}[breakatwhitespace=〈false)] 
http://www.vox.com/2016/9/7/12815076/america-uninsured-rate-dropped?source=socnet_tw_aca_20160907_bo_uninsured-rate-low_vox_3&utm_medium=social&utm_source=tw_bo&utm_campaign=socnet_tw_aca_20160907_bo_uninsured-rate-low_vox_3&utm_content=20160907_bo_uninsured-rate-low_vox_3
\end{lstlisting}
  \\ 
 \hline 
 0.0054 & 0.0032 & 1.6897 &
\begin{lstlisting}[breakatwhitespace=〈false)]  
http://www.vox.com/new-money/2016/10/21/13361414/median-wages?source=socnet_tw_econ_20161026_bo_wage-increase_durable_4&utm_medium=social&utm_source=tw_bo&utm_campaign=socnet_tw_econ_20161026_bo_wage-increase_durable_4&utm_content=20161026_bo_wage-increase_durable_4 
 \end{lstlisting}
 \\
 \hline 
 0.0035 & 0.0021 & 1.6897 &
 \begin{lstlisting}[breakatwhitespace=〈false)]
https://www.fisherwallace.com/?gclid=CNr-xNqt38UCFUU8gQodQHIAsQ 
\end{lstlisting}
\\ 
 \hline
 0.0008 & 0.0005 & 1.6897 &
 \begin{lstlisting}[breakatwhitespace=〈false)] 
 http://www.nme.com/features/taylor-swift-power-fame-and-the-future-the-full-nme-cover-interview-549 
  \end{lstlisting}
 \\
 \hline 
 0.0008 & 0.0005 & 1.6897 &
 \begin{lstlisting}[breakatwhitespace=〈false)] 
http://www.health.com/health/gallery/0,,20705881,00.html 
 \end{lstlisting}
 \\ 
 \hline 
 0.0029 & 0.0017 & 1.6897 &
 \begin{lstlisting}[breakatwhitespace=〈false)]
http://www.mtv.com/vma/winners
  \end{lstlisting}
  \\
 \hline 
 0.0081 & 0.0048 & 1.6897 &
 \begin{lstlisting}[breakatwhitespace=〈false)] 
http://www.vox.com/energy-and-environment/2016/9/30/13111088/cleantech-costs-falling-one-chart?source=socnet_tw_cc_20161006_bo_clean-energy_technology_1&utm_medium=social&utm_source=tw_bo&utm_campaign=socnet_tw_cc_20161006_bo_clean-energy_technology_1&utm_content=20161006_bo_clean-energy_technology_1
  \end{lstlisting}
 \\ 
  \hline 
 0.0012 & 0.0007 & 1.6897 &
\begin{lstlisting}[breakatwhitespace=〈false)] 
https://www.washingtonpost.com/news/energy-environment/wp/2016/10/10/climate-change-has-been-making-western-forest-fires-worse-for-decades-study-says/?source=socnet_tw_cc_20161027_bo_energy-environment_climate_5&utm_medium=social&utm_source=tw_bo&utm_campaign=socnet_tw_cc_20161027_bo_energy-environment_climate_5&utm_content=20161027_bo_energy-environment_climate_5 
\end{lstlisting}
\\
 \hline 
 0.0032 & 0.0019 & 1.6897 &
\begin{lstlisting}[breakatwhitespace=〈false)] 
http://www.npr.org/sections/thetwo-way/2016/08/31/492177267/obama-at-lake-tahoe-praises-conservation-efforts?source=socnet_tw_cc_20160906_bo_npr-article_conservation_2&utm_medium=socnet&utm_source=tw&utm_campaign=socnet_tw_cc_20160906_bo_npr-article_conservation_2&utm_content=20160906_bo_npr-article_conservation_2 
\end{lstlisting}
\\
 \hline
\end{longtable}

	


\subsection*{Included Files:}
map.txt
Folder ``blue'' contains all processed documents that contain the query term ``blue''.

\noindent 
Folder: timemaps that has all timemap.json* files,  

\section*{Question 3:}
Now rank the same 10 URIs from question 2, but this time 
by their PageRank.  Use any of the free PR estimaters on the web,
such as:

\burl{http://pr.eyedomain.com/}

\burl{http://www.prchecker.info/check_page_rank.php}

\burl{http://www.seocentro.com/tools/search-engines/pagerank.html}

\burl{http://www.checkpagerank.net/}

If you use these tools, you'll have to do so by hand (they have
anti-bot captchas), but there are only 10 to do.  Normalize the
values they give you to be from 0 to 1.0.  Use the same tool on all
10 (again, consistency is more important than accuracy).  Also
note that these tools typically report on the domain rather than
the page, so it's not entirely accurate.  

Create a table similar to Table 1:

Table 2.  10 hits for the term "shadow", ranked by PageRank.

PR	URI

-----------------------

0.9		http://bar.com/

0.5		http://foo.com/


Briefly compare and contrast the rankings produced in questions 2
and 3.

     
\subsection*{Answer:} 

Using \burl{http://pr.eyedomain.com/} to find the Google page rank for each of the URIs, and normalizing the results, the required table becomes:

Table 3:

\begin{longtable}{ |p{2cm}|p{12cm}| } 
 \hline
Page Rank & URI \\
 \hline 
  0.5 &
 \begin{lstlisting}[breakatwhitespace=〈false)]
 http://www.boohoo.com/
\end{lstlisting} 
 \\
 \hline 
 0.6 &
\begin{lstlisting}[breakatwhitespace=〈false)] 
http://www.vox.com/2016/9/7/12815076/america-uninsured-rate-dropped?source=socnet_tw_aca_20160907_bo_uninsured-rate-low_vox_3&utm_medium=social&utm_source=tw_bo&utm_campaign=socnet_tw_aca_20160907_bo_uninsured-rate-low_vox_3&utm_content=20160907_bo_uninsured-rate-low_vox_3
\end{lstlisting}
  \\ 
 \hline 
 0.6 &
\begin{lstlisting}[breakatwhitespace=〈false)]  
http://www.vox.com/new-money/2016/10/21/13361414/median-wages?source=socnet_tw_econ_20161026_bo_wage-increase_durable_4&utm_medium=social&utm_source=tw_bo&utm_campaign=socnet_tw_econ_20161026_bo_wage-increase_durable_4&utm_content=20161026_bo_wage-increase_durable_4 
 \end{lstlisting}
 \\
 \hline 
0.3 &
 \begin{lstlisting}[breakatwhitespace=〈false)]
https://www.fisherwallace.com/?gclid=CNr-xNqt38UCFUU8gQodQHIAsQ 
\end{lstlisting}
\\ 
 \hline
0.6 &
 \begin{lstlisting}[breakatwhitespace=〈false)] 
 http://www.nme.com/features/taylor-swift-power-fame-and-the-future-the-full-nme-cover-interview-549 
  \end{lstlisting}
 \\
 \hline 
0.7 &
 \begin{lstlisting}[breakatwhitespace=〈false)] 
http://www.health.com/health/gallery/0,,20705881,00.html 
 \end{lstlisting}
 \\ 
 \hline 
0.7 &
 \begin{lstlisting}[breakatwhitespace=〈false)]
http://www.mtv.com/vma/winners
  \end{lstlisting}
  \\
 \hline 
0.6 &
 \begin{lstlisting}[breakatwhitespace=〈false)] 
http://www.vox.com/energy-and-environment/2016/9/30/13111088/cleantech-costs-falling-one-chart?source=socnet_tw_cc_20161006_bo_clean-energy_technology_1&utm_medium=social&utm_source=tw_bo&utm_campaign=socnet_tw_cc_20161006_bo_clean-energy_technology_1&utm_content=20161006_bo_clean-energy_technology_1
  \end{lstlisting}
 \\ 
  \hline 
0.8 &
\begin{lstlisting}[breakatwhitespace=〈false)] 
https://www.washingtonpost.com/news/energy-environment/wp/2016/10/10/climate-change-has-been-making-western-forest-fires-worse-for-decades-study-says/?source=socnet_tw_cc_20161027_bo_energy-environment_climate_5&utm_medium=social&utm_source=tw_bo&utm_campaign=socnet_tw_cc_20161027_bo_energy-environment_climate_5&utm_content=20161027_bo_energy-environment_climate_5 
\end{lstlisting}
\\
 \hline 
0.8 &
\begin{lstlisting}[breakatwhitespace=〈false)] 
http://www.npr.org/sections/thetwo-way/2016/08/31/492177267/obama-at-lake-tahoe-praises-conservation-efforts?source=socnet_tw_cc_20160906_bo_npr-article_conservation_2&utm_medium=socnet&utm_source=tw&utm_campaign=socnet_tw_cc_20160906_bo_npr-article_conservation_2&utm_content=20160906_bo_npr-article_conservation_2 
\end{lstlisting}
\\
 \hline
\end{longtable}

Combining Table 2 and 3 generates the following table:

Table 4:

\begin{longtable}{ |p{1cm}|p{1cm}|p{1cm}|p{1cm}|p{9cm}| } 
 \hline
PR & TFIDF & TF & IDF & URI \\
 \hline 
 0.5 & 0.0022 & 0.0013 & 1.6897 &
 \begin{lstlisting}[breakatwhitespace=〈false)]
 http://www.boohoo.com/
\end{lstlisting} 
 \\
 \hline 
 0.6 & 0.0020 & 0.0012 & 1.6897 &
\begin{lstlisting}[breakatwhitespace=〈false)] 
http://www.vox.com/2016/9/7/12815076/america-uninsured-rate-dropped?source=socnet_tw_aca_20160907_bo_uninsured-rate-low_vox_3&utm_medium=social&utm_source=tw_bo&utm_campaign=socnet_tw_aca_20160907_bo_uninsured-rate-low_vox_3&utm_content=20160907_bo_uninsured-rate-low_vox_3
\end{lstlisting}
  \\ 
 \hline 
 0.6 & 0.0054 & 0.0032 & 1.6897 &
\begin{lstlisting}[breakatwhitespace=〈false)]  
http://www.vox.com/new-money/2016/10/21/13361414/median-wages?source=socnet_tw_econ_20161026_bo_wage-increase_durable_4&utm_medium=social&utm_source=tw_bo&utm_campaign=socnet_tw_econ_20161026_bo_wage-increase_durable_4&utm_content=20161026_bo_wage-increase_durable_4 
 \end{lstlisting}
 \\
 \hline 
 0.3 & 0.0035 & 0.0021 & 1.6897 &
 \begin{lstlisting}[breakatwhitespace=〈false)]
https://www.fisherwallace.com/?gclid=CNr-xNqt38UCFUU8gQodQHIAsQ 
\end{lstlisting}
\\ 
 \hline
0.6 & 0.0008 & 0.0005 & 1.6897 &
 \begin{lstlisting}[breakatwhitespace=〈false)] 
 http://www.nme.com/features/taylor-swift-power-fame-and-the-future-the-full-nme-cover-interview-549 
  \end{lstlisting}
 \\
 \hline 
 0.7 & 0.0008 & 0.0005 & 1.6897 &
 \begin{lstlisting}[breakatwhitespace=〈false)] 
http://www.health.com/health/gallery/0,,20705881,00.html 
 \end{lstlisting}
 \\ 
 \hline 
 0.7 & 0.0029 & 0.0017 & 1.6897 &
 \begin{lstlisting}[breakatwhitespace=〈false)]
http://www.mtv.com/vma/winners
  \end{lstlisting}
  \\
 \hline 
 0.6 & 0.0081 & 0.0048 & 1.6897 &
 \begin{lstlisting}[breakatwhitespace=〈false)] 
http://www.vox.com/energy-and-environment/2016/9/30/13111088/cleantech-costs-falling-one-chart?source=socnet_tw_cc_20161006_bo_clean-energy_technology_1&utm_medium=social&utm_source=tw_bo&utm_campaign=socnet_tw_cc_20161006_bo_clean-energy_technology_1&utm_content=20161006_bo_clean-energy_technology_1
  \end{lstlisting}
 \\ 
  \hline 
 0.8 & 0.0012 & 0.0007 & 1.6897 &
\begin{lstlisting}[breakatwhitespace=〈false)] 
https://www.washingtonpost.com/news/energy-environment/wp/2016/10/10/climate-change-has-been-making-western-forest-fires-worse-for-decades-study-says/?source=socnet_tw_cc_20161027_bo_energy-environment_climate_5&utm_medium=social&utm_source=tw_bo&utm_campaign=socnet_tw_cc_20161027_bo_energy-environment_climate_5&utm_content=20161027_bo_energy-environment_climate_5 
\end{lstlisting}
\\
 \hline 
 0.8 & 0.0032 & 0.0019 & 1.6897 &
\begin{lstlisting}[breakatwhitespace=〈false)] 
http://www.npr.org/sections/thetwo-way/2016/08/31/492177267/obama-at-lake-tahoe-praises-conservation-efforts?source=socnet_tw_cc_20160906_bo_npr-article_conservation_2&utm_medium=socnet&utm_source=tw&utm_campaign=socnet_tw_cc_20160906_bo_npr-article_conservation_2&utm_content=20160906_bo_npr-article_conservation_2 
\end{lstlisting}
\\
 \hline
\end{longtable}

From Table 4, we find that there is no obvious relationship between Google page rank and TFIDF for the document. Websites with higher page rank had both lower and higher TFIDF for their corresponding documents. It is also noticeable that various documents within the same domain have different TFIDF, high and low, but the same page rank (page rank is reported on the domain name rather than the document).


\end{document}
